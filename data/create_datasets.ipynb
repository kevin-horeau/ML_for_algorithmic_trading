{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install quandl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN8w0a9e_DLa",
        "outputId": "408d3b6f-1096-482f-e43f-a67320077500"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: quandl in /usr/local/lib/python3.10/dist-packages (3.7.0)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.10/dist-packages (from quandl) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from quandl) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from quandl) (2.31.0)\n",
            "Requirement already satisfied: inflection>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from quandl) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from quandl) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from quandl) (1.16.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from quandl) (10.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.14->quandl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.14->quandl) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->quandl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->quandl) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->quandl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->quandl) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n0_MnP7R_AyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1X976zy5oUtR"
      },
      "outputs": [],
      "source": [
        "# Imports & Settings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Standard library imports\n",
        "from pathlib import Path\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile, BadZipFile\n",
        "\n",
        "# Third-party library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as web\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Configure pandas to display full DataFrames\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Data Store Path\n",
        "\n",
        "This notebook uses a central location to store all downloaded and processed data. The `DATA_STORE` variable defines the path to an HDF5 file that will contain multiple datasets.\n",
        "\n",
        "**Note:** If you prefer to store the data in a different location, modify the `DATA_STORE` path below and update all references to this path in other notebooks."
      ],
      "metadata": {
        "id": "_2bfCdnv9X0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for the HDF5 file that will store all datasets\n",
        "DATA_STORE = Path('assets.h5')\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "DATA_STORE.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Data will be stored in: {DATA_STORE.absolute()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa9gatxz9BKs",
        "outputId": "420f1e74-8830-44ea-f2da-14e8839c7acf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data will be stored in: /content/assets.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quandl Wiki Prices Dataset\n",
        "\n",
        "### Background\n",
        "- Quandl, a financial data provider, was [acquired by NASDAQ](https://www.nasdaq.com/about/press-center/nasdaq-acquires-quandl-advance-use-alternative-data) in late 2018.\n",
        "- In 2021, NASDAQ [integrated Quandl's data platform](https://data.nasdaq.com/).\n",
        "\n",
        "### Dataset Description\n",
        "- Contains stock prices, dividends, and splits for 3000 US publicly-traded companies.\n",
        "- Available at: [NASDAQ Data Link](https://data.nasdaq.com/databases/WIKIP/documentation)\n",
        "- Historical data useful for demonstrating machine learning applications in finance.\n",
        "\n",
        "### Important Notes\n",
        "1. This dataset is no longer actively updated (last update: April 11, 2018).\n",
        "2. Use this data for learning and initial testing only.\n",
        "3. For production or current analysis, use up-to-date, professional-grade data sources.\n",
        "\n",
        "### Data Acquisition Steps\n",
        "1. Create a free [NASDAQ account](https://data.nasdaq.com/sign-up)\n",
        "2. [Download](https://data.nasdaq.com/tables/WIKIP/WIKI-PRICES/export) the entire WIKI/PRICES dataset\n",
        "3. Extract the .zip file\n",
        "4. Move the extracted file to this notebook's directory and rename it to `wiki_prices.csv`\n",
        "5. Run the code below to process and store the data in HDF5 format"
      ],
      "metadata": {
        "id": "gwlgK0W-93zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def get_wiki_prices_file():\n",
        "    \"\"\"\n",
        "    Attempt to get the WIKI Prices file from Google Drive, unzip it, and rename it.\n",
        "    If unsuccessful, prompt the user to download manually.\n",
        "\n",
        "    Returns:\n",
        "    Path: Path to the CSV file\n",
        "    \"\"\"\n",
        "    drive_file_path = Path('/content/drive/MyDrive/ML4T/WIKI_PRICES_212b326a081eacca455e13140d7bb9db.zip')\n",
        "\n",
        "    if drive_file_path.exists():\n",
        "        print(\"Found WIKI Prices zip file in Google Drive. Extracting...\")\n",
        "        with zipfile.ZipFile(drive_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "\n",
        "        # Find the extracted CSV file\n",
        "        csv_file = next(Path('.').glob('*.csv'), None)\n",
        "        if csv_file:\n",
        "            # Rename the file\n",
        "            new_file_name = 'wiki_prices.csv'\n",
        "            csv_file.rename(new_file_name)\n",
        "            print(f\"File extracted and renamed to {new_file_name}\")\n",
        "            return Path(new_file_name)\n",
        "        else:\n",
        "            print(\"Couldn't find the CSV file in the extracted contents.\")\n",
        "\n",
        "    print(\"WIKI Prices file not found in Google Drive or extraction failed.\")\n",
        "    print(\"Please download the file manually using the following steps:\")\n",
        "    print(\"1. Go to https://data.nasdaq.com/tables/WIKIP/WIKI-PRICES/export\")\n",
        "    print(\"2. Download the entire WIKI/PRICES data\")\n",
        "    print(\"3. Extract the .zip file\")\n",
        "    print(\"4. Move the extracted CSV file to this notebook's directory and rename it to 'wiki_prices.csv'\")\n",
        "\n",
        "    return None\n",
        "\n",
        "def process_quandl_wiki_prices(file_path='wiki_prices.csv'):\n",
        "    \"\"\"\n",
        "    Process the Quandl WIKI Prices dataset from a CSV file and store it in HDF5 format.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Path to the CSV file containing the WIKI Prices data.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Processed DataFrame of WIKI Prices data.\n",
        "    \"\"\"\n",
        "    csv_path = Path(file_path)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please ensure you've downloaded the CSV file and placed it in the correct directory.\")\n",
        "\n",
        "    print(f\"Processing {file_path}...\")\n",
        "\n",
        "    # Read CSV file, parse dates, and set multi-index\n",
        "    df = pd.read_csv(csv_path,\n",
        "                     parse_dates=['date'],\n",
        "                     index_col=['date', 'ticker'],\n",
        "                     infer_datetime_format=True)\n",
        "\n",
        "    # Sort the index for efficient data access\n",
        "    df = df.sort_index()\n",
        "\n",
        "    # Print DataFrame info for verification\n",
        "    print(\"\\nWIKI Prices Dataset Info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Print null value counts separately\n",
        "    print(\"\\nNull value counts:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Store the data in HDF5 format\n",
        "    with pd.HDFStore(DATA_STORE) as store:\n",
        "        store.put('quandl/wiki/prices', df)\n",
        "        print(f\"\\nData stored in {DATA_STORE} under 'quandl/wiki/prices'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Main execution\n",
        "try:\n",
        "    csv_file_path = get_wiki_prices_file()\n",
        "    if csv_file_path:\n",
        "        wiki_prices_df = process_quandl_wiki_prices(csv_file_path)\n",
        "        print(\"\\nQuandl WIKI Prices data processed successfully.\")\n",
        "    else:\n",
        "        print(\"\\nPlease run this cell again after manually downloading and placing the CSV file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNaowrKHBsB9",
        "outputId": "290c45c6-33aa-4d66-a18d-d9295cfdf258"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found WIKI Prices zip file in Google Drive. Extracting...\n",
            "File extracted and renamed to wiki_prices.csv\n",
            "Processing wiki_prices.csv...\n",
            "An error occurred: Missing column provided to 'parse_dates': 'date'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wiki Prices Metadata\n",
        "\n",
        "def process_wiki_stocks_metadata(file_path='wiki_stocks.csv'):\n",
        "    \"\"\"\n",
        "    Process the Wiki Stocks metadata and store it in HDF5 format.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Path to the CSV file containing the Wiki Stocks metadata.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Processed DataFrame of Wiki Stocks metadata.\n",
        "    \"\"\"\n",
        "    csv_path = Path(file_path)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please ensure you've downloaded the CSV file and placed it in the correct directory.\")\n",
        "\n",
        "    print(f\"Processing {file_path}...\")\n",
        "\n",
        "    # Read CSV file\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Print DataFrame info for verification\n",
        "    print(\"\\nWiki Stocks Metadata Info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Print sample data\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Store the data in HDF5 format\n",
        "    with pd.HDFStore(DATA_STORE) as store:\n",
        "        store.put('quandl/wiki/stocks', df.set_index('code'))\n",
        "        print(f\"\\nData stored in {DATA_STORE} under 'quandl/wiki/stocks'\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Gw0L64wRBr9x"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# S&P 500 Prices\n",
        "\n",
        "import pandas_datareader.data as web\n",
        "from datetime import datetime\n",
        "\n",
        "def fetch_sp500_prices(start_date='2010-01-01', end_date=None):\n",
        "    \"\"\"\n",
        "    Fetch S&P 500 prices from FRED and store them in HDF5 format.\n",
        "\n",
        "    Args:\n",
        "    start_date (str): Start date for data fetch (default: '2010-01-01')\n",
        "    end_date (str): End date for data fetch (default: current date)\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame of S&P 500 prices\n",
        "    \"\"\"\n",
        "    if end_date is None:\n",
        "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    print(f\"Fetching S&P 500 prices from {start_date} to {end_date}...\")\n",
        "\n",
        "    try:\n",
        "        df = web.DataReader(name='SP500', data_source='fred', start=start_date, end=end_date)\n",
        "        df = df.rename(columns={'SP500': 'close'})\n",
        "\n",
        "        print(\"\\nS&P 500 Prices Info:\")\n",
        "        print(df.info())\n",
        "\n",
        "        print(\"\\nSample data:\")\n",
        "        print(df.head())\n",
        "\n",
        "        with pd.HDFStore(DATA_STORE) as store:\n",
        "            store.put('sp500/fred', df)\n",
        "            print(f\"\\nData stored in {DATA_STORE} under 'sp500/fred'\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fetching S&P 500 prices: {str(e)}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "Wqjfk0qoBr07"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive if not already mounted\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Google Drive is already mounted.\")\n",
        "\n",
        "def check_and_move_wiki_stocks():\n",
        "    \"\"\"\n",
        "    Check for wiki_stocks.csv in Google Drive and move it to the current directory if found.\n",
        "    \"\"\"\n",
        "    source_path = Path('/content/drive/MyDrive/ML4T/wiki_stocks.csv')\n",
        "    destination_path = Path('wiki_stocks.csv')\n",
        "\n",
        "    if source_path.exists():\n",
        "        print(f\"Found wiki_stocks.csv in Google Drive. Moving to {destination_path}\")\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        print(\"File moved successfully.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"wiki_stocks.csv not found in Google Drive.\")\n",
        "        return False\n",
        "\n",
        "# Check for wiki_stocks.csv and move it if found\n",
        "file_moved = check_and_move_wiki_stocks()\n",
        "\n",
        "if file_moved:\n",
        "    # Process the Wiki Stocks metadata\n",
        "    try:\n",
        "        wiki_stocks_df = process_wiki_stocks_metadata()\n",
        "        print(\"\\nWiki Stocks metadata processed successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the Wiki Stocks metadata: {str(e)}\")\n",
        "else:\n",
        "    print(\"\\nPlease ensure you have the 'wiki_stocks.csv' file in your Google Drive 'ML4T' folder.\")\n",
        "    print(\"If the file is not available, you may need to obtain it from another source.\")\n",
        "\n",
        "# Proceed with S&P 500 Prices fetching\n",
        "sp500_prices = fetch_sp500_prices()\n",
        "\n",
        "if sp500_prices is not None:\n",
        "    print(\"\\nS&P 500 prices fetched and stored successfully.\")\n",
        "else:\n",
        "    print(\"\\nFailed to fetch S&P 500 prices. Please check your internet connection and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m_qrnv5FbZy",
        "outputId": "58c75c85-ad10-4492-b6fd-fdca67cba51d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found wiki_stocks.csv in Google Drive. Moving to wiki_stocks.csv\n",
            "File moved successfully.\n",
            "Processing wiki_stocks.csv...\n",
            "\n",
            "Wiki Stocks Metadata Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3199 entries, 0 to 3198\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   code    3199 non-null   object\n",
            " 1   name    3199 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 50.1+ KB\n",
            "None\n",
            "\n",
            "Sample data:\n",
            "   code                          name\n",
            "0     A     Agilent Technologies Inc.\n",
            "1    AA                    Alcoa Inc.\n",
            "2   AAL  American Airlines Group Inc.\n",
            "3  AAMC   Altisource Asset Management\n",
            "4   AAN                  Aaron's Inc.\n",
            "\n",
            "Data stored in assets.h5 under 'quandl/wiki/stocks'\n",
            "\n",
            "Wiki Stocks metadata processed successfully.\n",
            "Fetching S&P 500 prices from 2010-01-01 to 2024-07-25...\n",
            "\n",
            "S&P 500 Prices Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 2609 entries, 2014-07-25 to 2024-07-24\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   close   2516 non-null   float64\n",
            "dtypes: float64(1)\n",
            "memory usage: 40.8 KB\n",
            "None\n",
            "\n",
            "Sample data:\n",
            "              close\n",
            "DATE               \n",
            "2014-07-25  1978.34\n",
            "2014-07-28  1978.91\n",
            "2014-07-29  1969.95\n",
            "2014-07-30  1970.07\n",
            "2014-07-31  1930.67\n",
            "\n",
            "Data stored in assets.h5 under 'sp500/fred'\n",
            "\n",
            "S&P 500 prices fetched and stored successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "def check_and_move_sp500_stooq_file():\n",
        "    \"\"\"\n",
        "    Check for ^spx_d_daily.csv in Google Drive and move it to the current directory if found.\n",
        "    \"\"\"\n",
        "    source_path = Path('/content/drive/MyDrive/ML4T/^spx_d_daily.csv')\n",
        "    destination_path = Path('^spx_d.csv')\n",
        "\n",
        "    if source_path.exists():\n",
        "        print(f\"Found ^spx_d_daily.csv in Google Drive. Moving to {destination_path}\")\n",
        "        shutil.copy(source_path, destination_path)\n",
        "        print(\"File moved successfully.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"^spx_d_daily.csv not found in Google Drive.\")\n",
        "        return False\n",
        "\n",
        "def process_sp500_stooq(file_path='^spx_d.csv', start_year='1950', end_year='2019'):\n",
        "    \"\"\"\n",
        "    Process S&P 500 data from Stooq.com and store it in HDF5 format.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): Path to the CSV file containing S&P 500 data from Stooq.\n",
        "    start_year (str): Start year for data processing.\n",
        "    end_year (str): End year for data processing.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Processed DataFrame of S&P 500 data.\n",
        "    \"\"\"\n",
        "    csv_path = Path(file_path)\n",
        "    if not csv_path.exists():\n",
        "        raise FileNotFoundError(f\"The file {file_path} does not exist. Please download the S&P 500 data from Stooq.com.\")\n",
        "\n",
        "    print(f\"Processing {file_path}...\")\n",
        "\n",
        "    # Read CSV file, parse dates, set index, and filter date range\n",
        "    df = (pd.read_csv(csv_path, index_col=0, parse_dates=True)\n",
        "          .loc[f'{start_year}':f'{end_year}']\n",
        "          .rename(columns=str.lower))\n",
        "\n",
        "    # Print DataFrame info for verification\n",
        "    print(\"\\nS&P 500 (Stooq) Dataset Info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Print sample data\n",
        "    print(\"\\nSample data:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Store the data in HDF5 format\n",
        "    with pd.HDFStore(DATA_STORE) as store:\n",
        "        store.put('sp500/stooq', df)\n",
        "        print(f\"\\nData stored in {DATA_STORE} under 'sp500/stooq'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# First, check and move the file if it's in Google Drive\n",
        "file_moved = check_and_move_sp500_stooq_file()\n",
        "\n",
        "# Process the S&P 500 data from Stooq\n",
        "if file_moved or Path('^spx_d.csv').exists():\n",
        "    try:\n",
        "        sp500_stooq_df = process_sp500_stooq()\n",
        "        print(\"\\nS&P 500 data from Stooq processed successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the S&P 500 data: {str(e)}\")\n",
        "else:\n",
        "    print(\"\\nS&P 500 data file (^spx_d.csv) not found.\")\n",
        "    print(\"Please download the S&P 500 data from Stooq.com and place it in the notebook directory or your Google Drive ML4T folder as '^spx_d_daily.csv'.\")\n",
        "    print(\"You can download the data from: https://stooq.com/q/d/?s=%5Espx\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44RNAzDgFiRk",
        "outputId": "d4a58ac9-5a80-40e2-912b-6a44497f6acb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found ^spx_d_daily.csv in Google Drive. Moving to ^spx_d.csv\n",
            "File moved successfully.\n",
            "Processing ^spx_d.csv...\n",
            "\n",
            "S&P 500 (Stooq) Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 17700 entries, 1950-01-03 to 2019-12-31\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   open    17700 non-null  float64\n",
            " 1   high    17700 non-null  float64\n",
            " 2   low     17700 non-null  float64\n",
            " 3   close   17700 non-null  float64\n",
            " 4   volume  17700 non-null  float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 829.7 KB\n",
            "None\n",
            "\n",
            "Sample data:\n",
            "             open   high    low  close     volume\n",
            "Date                                             \n",
            "1950-01-03  16.66  16.66  16.66  16.66   700000.0\n",
            "1950-01-04  16.85  16.85  16.85  16.85  1050000.0\n",
            "1950-01-05  16.93  16.93  16.93  16.93  1416667.0\n",
            "1950-01-06  16.98  16.98  16.98  16.98  1116667.0\n",
            "1950-01-07  17.09  17.09  17.09  17.09  1116667.0\n",
            "\n",
            "Data stored in assets.h5 under 'sp500/stooq'\n",
            "\n",
            "S&P 500 data from Stooq processed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "def fetch_sp500_constituents():\n",
        "    \"\"\"\n",
        "    Fetch current S&P 500 constituents from Wikipedia and store in HDF5 format.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame of S&P 500 constituents\n",
        "    \"\"\"\n",
        "    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
        "\n",
        "    print(\"Fetching S&P 500 constituents from Wikipedia...\")\n",
        "\n",
        "    try:\n",
        "        # Read HTML tables from Wikipedia\n",
        "        tables = pd.read_html(url, header=0)\n",
        "        df = tables[0]\n",
        "\n",
        "        # Print the columns to see what we're dealing with\n",
        "        print(\"Columns in the fetched data:\")\n",
        "        print(df.columns)\n",
        "\n",
        "        # Dynamically rename columns based on the number of columns\n",
        "        if len(df.columns) == 9:\n",
        "            df.columns = ['ticker', 'name', 'sec_filings', 'gics_sector', 'gics_sub_industry',\n",
        "                          'headquarters', 'date_added', 'cik', 'founded']\n",
        "        elif len(df.columns) == 8:\n",
        "            df.columns = ['ticker', 'name', 'sec_filings', 'gics_sector', 'gics_sub_industry',\n",
        "                          'headquarters', 'date_added', 'cik']\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected number of columns: {len(df.columns)}\")\n",
        "\n",
        "        # Drop 'sec_filings' column and set 'ticker' as index\n",
        "        df = df.drop('sec_filings', axis=1).set_index('ticker')\n",
        "\n",
        "        print(\"\\nS&P 500 Constituents Info:\")\n",
        "        print(df.info())\n",
        "\n",
        "        print(\"\\nSample data:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Store the data in HDF5 format\n",
        "        with pd.HDFStore(DATA_STORE) as store:\n",
        "            store.put('sp500/stocks', df)\n",
        "            print(f\"\\nData stored in {DATA_STORE} under 'sp500/stocks'\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while fetching S&P 500 constituents: {str(e)}\")\n",
        "        print(\"Columns in the fetched data:\")\n",
        "        print(df.columns if 'df' in locals() else \"DataFrame not created\")\n",
        "        return None\n",
        "\n",
        "# Fetch S&P 500 constituents\n",
        "sp500_constituents = fetch_sp500_constituents()\n",
        "\n",
        "if sp500_constituents is not None:\n",
        "    print(\"\\nS&P 500 constituents fetched and stored successfully.\")\n",
        "else:\n",
        "    print(\"\\nFailed to fetch S&P 500 constituents. Please check the error message above for details.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWDFQvJ9Ivc3",
        "outputId": "32e0ea2c-2df7-47f0-9ed1-190ceeed8df7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching S&P 500 constituents from Wikipedia...\n",
            "Columns in the fetched data:\n",
            "Index(['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry',\n",
            "       'Headquarters Location', 'Date added', 'CIK', 'Founded'],\n",
            "      dtype='object')\n",
            "\n",
            "S&P 500 Constituents Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 503 entries, MMM to ZTS\n",
            "Data columns (total 6 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   name               503 non-null    object\n",
            " 1   gics_sector        503 non-null    object\n",
            " 2   gics_sub_industry  503 non-null    object\n",
            " 3   headquarters       503 non-null    object\n",
            " 4   date_added         503 non-null    int64 \n",
            " 5   cik                503 non-null    object\n",
            "dtypes: int64(1), object(5)\n",
            "memory usage: 27.5+ KB\n",
            "None\n",
            "\n",
            "Sample data:\n",
            "               name                     gics_sector        gics_sub_industry headquarters  date_added          cik\n",
            "ticker                                                                                                            \n",
            "MMM              3M        Industrial Conglomerates    Saint Paul, Minnesota   1957-03-04       66740         1902\n",
            "AOS     A. O. Smith               Building Products     Milwaukee, Wisconsin   2017-07-26       91142         1916\n",
            "ABT          Abbott           Health Care Equipment  North Chicago, Illinois   1957-03-04        1800         1888\n",
            "ABBV         AbbVie                   Biotechnology  North Chicago, Illinois   2012-12-31     1551152  2013 (1888)\n",
            "ACN       Accenture  IT Consulting & Other Services          Dublin, Ireland   2011-07-06     1467373         1989\n",
            "\n",
            "Data stored in assets.h5 under 'sp500/stocks'\n",
            "\n",
            "S&P 500 constituents fetched and stored successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "def move_and_rename_exchange_files():\n",
        "    \"\"\"\n",
        "    Move and rename exchange files from Google Drive to the current directory.\n",
        "    \"\"\"\n",
        "    exchanges = ['AMEX', 'NYSE', 'NASDAQ']\n",
        "    renamed_files = []\n",
        "\n",
        "    for exchange in exchanges:\n",
        "        source_path = Path(f'/content/drive/MyDrive/ML4T/nasdaq_screener_{exchange}.csv')\n",
        "        destination_path = Path(f'{exchange.lower()}_stocks.csv')\n",
        "\n",
        "        if source_path.exists():\n",
        "            print(f\"Found {source_path.name} in Google Drive. Moving to {destination_path}\")\n",
        "            shutil.copy(source_path, destination_path)\n",
        "            renamed_files.append(destination_path)\n",
        "            print(f\"File moved and renamed successfully to {destination_path}\")\n",
        "        else:\n",
        "            print(f\"{source_path.name} not found in Google Drive.\")\n",
        "\n",
        "    return renamed_files"
      ],
      "metadata": {
        "id": "bIEQC3ZXJnf3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def process_us_equities_metadata(file_paths):\n",
        "    \"\"\"\n",
        "    Process US equities metadata from multiple CSV files, including market cap conversion.\n",
        "\n",
        "    Args:\n",
        "    file_paths (list): List of paths to the CSV files containing US equities metadata.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Processed DataFrame of US equities metadata.\n",
        "    \"\"\"\n",
        "    dfs = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        print(f\"Processing {file_path}...\")\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Columns in {file_path}:\")\n",
        "        print(df.columns)\n",
        "        dfs.append(df)\n",
        "\n",
        "    # Combine data from all exchanges\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Clean up the data\n",
        "    df = df.rename(columns=str.lower)\n",
        "\n",
        "    # Check if 'ticker' column exists, if not, try to find a suitable column\n",
        "    if 'ticker' not in df.columns:\n",
        "        possible_ticker_columns = ['symbol', 'stock symbol', 'ticker symbol']\n",
        "        for col in possible_ticker_columns:\n",
        "            if col in df.columns:\n",
        "                df = df.rename(columns={col: 'ticker'})\n",
        "                break\n",
        "        else:\n",
        "            raise KeyError(f\"Could not find a suitable column for ticker. Available columns are: {df.columns}\")\n",
        "\n",
        "    df = df.set_index('ticker')\n",
        "    df = df[~df.index.duplicated(keep='first')]\n",
        "\n",
        "    print(\"\\nUS Equities Metadata Info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    print(\"\\nSample data before market cap conversion:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # Convert market cap to numeric\n",
        "    def convert_market_cap(value):\n",
        "        if pd.isna(value):\n",
        "            return np.nan\n",
        "        if isinstance(value, (int, float)):\n",
        "            return value\n",
        "        value = str(value).replace('$', '')\n",
        "        if value.endswith('T'):\n",
        "            return float(value[:-1]) * 1e12\n",
        "        elif value.endswith('B'):\n",
        "            return float(value[:-1]) * 1e9\n",
        "        elif value.endswith('M'):\n",
        "            return float(value[:-1]) * 1e6\n",
        "        else:\n",
        "            try:\n",
        "                return float(value)\n",
        "            except ValueError:\n",
        "                return np.nan\n",
        "\n",
        "    # Check if 'market cap' column exists, if not, try to find a suitable column\n",
        "    market_cap_column = 'market cap'\n",
        "    if market_cap_column not in df.columns:\n",
        "        possible_market_cap_columns = ['marketcap', 'market capitalization', 'cap']\n",
        "        for col in possible_market_cap_columns:\n",
        "            if col in df.columns:\n",
        "                market_cap_column = col\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Warning: Could not find a suitable column for market cap. Available columns are: {df.columns}\")\n",
        "            market_cap_column = None\n",
        "\n",
        "    if market_cap_column:\n",
        "        df[market_cap_column] = df[market_cap_column].apply(convert_market_cap)\n",
        "\n",
        "        print(\"\\nSample data after market cap conversion:\")\n",
        "        print(df.head())\n",
        "\n",
        "        print(\"\\nMarket Cap Statistics:\")\n",
        "        print(df[market_cap_column].describe())\n",
        "    else:\n",
        "        print(\"Market cap conversion skipped due to missing column.\")\n",
        "\n",
        "    # Store the data in HDF5 format\n",
        "    with pd.HDFStore(DATA_STORE) as store:\n",
        "        store.put('us_equities/stocks', df)\n",
        "        print(f\"\\nData stored in {DATA_STORE} under 'us_equities/stocks'\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Use this function after moving and renaming the files\n",
        "renamed_files = move_and_rename_exchange_files()\n",
        "\n",
        "if renamed_files:\n",
        "    # Process US equities metadata\n",
        "    us_equities_metadata = process_us_equities_metadata(renamed_files)\n",
        "    print(\"\\nUS equities metadata processed and stored successfully.\")\n",
        "else:\n",
        "    print(\"\\nNo exchange files found in Google Drive. Please ensure you've uploaded the files with the correct names.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsxC-N7yQXXX",
        "outputId": "871ec164-dbb3-4df9-b4aa-141b3e7ede87"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found nasdaq_screener_AMEX.csv in Google Drive. Moving to amex_stocks.csv\n",
            "File moved and renamed successfully to amex_stocks.csv\n",
            "Found nasdaq_screener_NYSE.csv in Google Drive. Moving to nyse_stocks.csv\n",
            "File moved and renamed successfully to nyse_stocks.csv\n",
            "Found nasdaq_screener_NASDAQ.csv in Google Drive. Moving to nasdaq_stocks.csv\n",
            "File moved and renamed successfully to nasdaq_stocks.csv\n",
            "Processing amex_stocks.csv...\n",
            "Columns in amex_stocks.csv:\n",
            "Index(['Symbol', 'Name', 'Last Sale', 'Net Change', '% Change', 'Market Cap',\n",
            "       'Country', 'IPO Year', 'Volume', 'Sector', 'Industry'],\n",
            "      dtype='object')\n",
            "Processing nyse_stocks.csv...\n",
            "Columns in nyse_stocks.csv:\n",
            "Index(['Symbol', 'Name', 'Last Sale', 'Net Change', '% Change', 'Market Cap',\n",
            "       'Country', 'IPO Year', 'Volume', 'Sector', 'Industry'],\n",
            "      dtype='object')\n",
            "Processing nasdaq_stocks.csv...\n",
            "Columns in nasdaq_stocks.csv:\n",
            "Index(['Symbol', 'Name', 'Last Sale', 'Net Change', '% Change', 'Market Cap',\n",
            "       'Country', 'IPO Year', 'Volume', 'Sector', 'Industry'],\n",
            "      dtype='object')\n",
            "\n",
            "US Equities Metadata Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 7071 entries, AAMC to ZYXI\n",
            "Data columns (total 10 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   name        7071 non-null   object \n",
            " 1   last sale   7071 non-null   object \n",
            " 2   net change  7071 non-null   float64\n",
            " 3   % change    7070 non-null   object \n",
            " 4   market cap  6657 non-null   float64\n",
            " 5   country     6799 non-null   object \n",
            " 6   ipo year    3988 non-null   float64\n",
            " 7   volume      7071 non-null   int64  \n",
            " 8   sector      6537 non-null   object \n",
            " 9   industry    6536 non-null   object \n",
            "dtypes: float64(3), int64(1), object(6)\n",
            "memory usage: 865.7+ KB\n",
            "None\n",
            "\n",
            "Sample data before market cap conversion:\n",
            "                                                     name last sale  net change % change   market cap        country  ipo year  volume                  sector                         industry\n",
            "ticker                                                                                                                                                                                         \n",
            "AAMC                 Altisource Asset Management Corp Com     $2.85        0.35   14.00%    7280359.0  United States       NaN    2381                 Finance                      Real Estate\n",
            "ACU                 Acme United Corporation. Common Stock    $39.53       -0.02  -0.051%  144754116.0  United States    1988.0   11105  Consumer Discretionary  Industrial Machinery/Components\n",
            "ADRT    Ault Disruptive Technologies Corporation Commo...    $16.21       -1.79  -9.944%  233018750.0            NaN    2022.0   37827                 Finance                     Blank Checks\n",
            "AE             Adams Resources & Energy Inc. Common Stock    $27.21        0.21   0.778%   69850900.0  United States    1980.0    2444                  Energy           Oil Refining/Marketing\n",
            "AEF     abrdn Emerging Markets Equity Income Fund Inc....     $5.21       -0.10  -1.883%  559358625.0  United States       NaN   19459                 Finance       Finance/Investors Services\n",
            "\n",
            "Sample data after market cap conversion:\n",
            "                                                     name last sale  net change % change   market cap        country  ipo year  volume                  sector                         industry\n",
            "ticker                                                                                                                                                                                         \n",
            "AAMC                 Altisource Asset Management Corp Com     $2.85        0.35   14.00%    7280359.0  United States       NaN    2381                 Finance                      Real Estate\n",
            "ACU                 Acme United Corporation. Common Stock    $39.53       -0.02  -0.051%  144754116.0  United States    1988.0   11105  Consumer Discretionary  Industrial Machinery/Components\n",
            "ADRT    Ault Disruptive Technologies Corporation Commo...    $16.21       -1.79  -9.944%  233018750.0            NaN    2022.0   37827                 Finance                     Blank Checks\n",
            "AE             Adams Resources & Energy Inc. Common Stock    $27.21        0.21   0.778%   69850900.0  United States    1980.0    2444                  Energy           Oil Refining/Marketing\n",
            "AEF     abrdn Emerging Markets Equity Income Fund Inc....     $5.21       -0.10  -1.883%  559358625.0  United States       NaN   19459                 Finance       Finance/Investors Services\n",
            "\n",
            "Market Cap Statistics:\n",
            "count    6.657000e+03\n",
            "mean     1.108710e+10\n",
            "std      8.962694e+10\n",
            "min      0.000000e+00\n",
            "25%      3.182024e+07\n",
            "50%      4.592665e+08\n",
            "75%      3.188610e+09\n",
            "max      3.351110e+12\n",
            "Name: market cap, dtype: float64\n",
            "\n",
            "Data stored in assets.h5 under 'us_equities/stocks'\n",
            "\n",
            "US equities metadata processed and stored successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def fetch_and_save_mnist():\n",
        "    \"\"\"\n",
        "    Fetch MNIST data from OpenML and save it locally.\n",
        "    \"\"\"\n",
        "    print(\"Fetching MNIST data from OpenML...\")\n",
        "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
        "\n",
        "    print(\"MNIST data fetched. Saving locally...\")\n",
        "\n",
        "    mnist_path = Path('mnist')\n",
        "    mnist_path.mkdir(exist_ok=True)\n",
        "\n",
        "    np.save(mnist_path / 'data.npy', mnist.data.astype(np.uint8))\n",
        "    np.save(mnist_path / 'target.npy', mnist.target.astype(np.uint8))\n",
        "\n",
        "    with open(mnist_path / 'description.txt', 'w') as f:\n",
        "        f.write(mnist.DESCR)\n",
        "\n",
        "    print(\"MNIST data saved locally in 'mnist' directory.\")\n",
        "    print(f\"Data shape: {mnist.data.shape}\")\n",
        "    print(f\"Target shape: {mnist.target.shape}\")\n",
        "\n",
        "# Fetch and save MNIST data\n",
        "fetch_and_save_mnist()\n",
        "\n",
        "# Verify saved files\n",
        "mnist_path = Path('mnist')\n",
        "if mnist_path.exists():\n",
        "    print(\"\\nVerifying saved MNIST files:\")\n",
        "    for file in ['data.npy', 'target.npy', 'description.txt']:\n",
        "        if (mnist_path / file).exists():\n",
        "            print(f\"  {file} exists\")\n",
        "        else:\n",
        "            print(f\"  {file} is missing\")\n",
        "else:\n",
        "    print(\"\\nMNIST directory not found. Data may not have been saved correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIdCxahSQmi-",
        "outputId": "0c237651-39fb-4571-fad9-b538f014ca53"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching MNIST data from OpenML...\n",
            "MNIST data fetched. Saving locally...\n",
            "MNIST data saved locally in 'mnist' directory.\n",
            "Data shape: (70000, 784)\n",
            "Target shape: (70000,)\n",
            "\n",
            "Verifying saved MNIST files:\n",
            "  data.npy exists\n",
            "  target.npy exists\n",
            "  description.txt exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_and_save_fashion_mnist():\n",
        "    \"\"\"\n",
        "    Fetch Fashion MNIST data from OpenML and save it locally.\n",
        "    \"\"\"\n",
        "    print(\"Fetching Fashion MNIST data from OpenML...\")\n",
        "    fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
        "\n",
        "    print(\"Fashion MNIST data fetched. Saving locally...\")\n",
        "\n",
        "    fashion_path = Path('fashion_mnist')\n",
        "    fashion_path.mkdir(exist_ok=True)\n",
        "\n",
        "    np.save(fashion_path / 'data.npy', fashion_mnist.data.astype(np.uint8))\n",
        "    np.save(fashion_path / 'target.npy', fashion_mnist.target.astype(np.uint8))\n",
        "\n",
        "    with open(fashion_path / 'description.txt', 'w') as f:\n",
        "        f.write(fashion_mnist.DESCR)\n",
        "\n",
        "    # Save label dictionary\n",
        "    label_dict = {\n",
        "        0: 'T-shirt/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat',\n",
        "        5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'\n",
        "    }\n",
        "    pd.Series(label_dict).to_csv(fashion_path / 'label_dict.csv', header=False)\n",
        "\n",
        "    print(\"Fashion MNIST data saved locally in 'fashion_mnist' directory.\")\n",
        "    print(f\"Data shape: {fashion_mnist.data.shape}\")\n",
        "    print(f\"Target shape: {fashion_mnist.target.shape}\")\n",
        "\n",
        "# Fetch and save Fashion MNIST data\n",
        "fetch_and_save_fashion_mnist()\n",
        "\n",
        "# Verify saved files\n",
        "fashion_path = Path('fashion_mnist')\n",
        "if fashion_path.exists():\n",
        "    print(\"\\nVerifying saved Fashion MNIST files:\")\n",
        "    for file in ['data.npy', 'target.npy', 'description.txt', 'label_dict.csv']:\n",
        "        if (fashion_path / file).exists():\n",
        "            print(f\"  {file} exists\")\n",
        "        else:\n",
        "            print(f\"  {file} is missing\")\n",
        "else:\n",
        "    print(\"\\nFashion MNIST directory not found. Data may not have been saved correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPPbP07MRQyU",
        "outputId": "f816687a-c767-4752-8213-ccf96e996290"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Fashion MNIST data from OpenML...\n",
            "Fashion MNIST data fetched. Saving locally...\n",
            "Fashion MNIST data saved locally in 'fashion_mnist' directory.\n",
            "Data shape: (70000, 784)\n",
            "Target shape: (70000,)\n",
            "\n",
            "Verifying saved Fashion MNIST files:\n",
            "  data.npy exists\n",
            "  target.npy exists\n",
            "  description.txt exists\n",
            "  label_dict.csv exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_datareader.data as web\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "\n",
        "def fetch_bond_price_indexes(start_date='2000-01-01', end_date=None):\n",
        "    \"\"\"\n",
        "    Fetch bond price indexes from FRED.\n",
        "\n",
        "    Args:\n",
        "    start_date (str): Start date for data fetch (default: '2000-01-01')\n",
        "    end_date (str): End date for data fetch (default: current date)\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame of bond price indexes\n",
        "    \"\"\"\n",
        "    if end_date is None:\n",
        "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "    print(f\"Fetching bond price indexes from {start_date} to {end_date}...\")\n",
        "\n",
        "    securities = {\n",
        "        'BAMLCC0A0CMTRIV'   : 'US Corp Master TRI',\n",
        "        'BAMLHYH0A0HYM2TRIV': 'US High Yield TRI',\n",
        "        'BAMLEMCBPITRIV'    : 'Emerging Markets Corporate Plus TRI',\n",
        "        'DGS10'             : '10-Year Treasury CMR',\n",
        "    }\n",
        "\n",
        "    dfs = []\n",
        "\n",
        "    for ticker, name in securities.items():\n",
        "        try:\n",
        "            df = web.DataReader(ticker, 'fred', start=start_date, end=end_date)\n",
        "            df.columns = [name]\n",
        "            dfs.append(df)\n",
        "            print(f\"Successfully fetched data for {name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data for {name}: {str(e)}\")\n",
        "\n",
        "    if not dfs:\n",
        "        print(\"Failed to fetch data for any securities.\")\n",
        "        return None\n",
        "\n",
        "    # Combine all dataframes\n",
        "    combined_df = pd.concat(dfs, axis=1)\n",
        "\n",
        "    # Resample to business days and forward fill missing values\n",
        "    combined_df = combined_df.resample('B').ffill()\n",
        "\n",
        "    print(\"\\nBond Price Indexes Info:\")\n",
        "    print(combined_df.info())\n",
        "\n",
        "    print(\"\\nSample data:\")\n",
        "    print(combined_df.head())\n",
        "\n",
        "    # Store the data in HDF5 format\n",
        "    with pd.HDFStore(DATA_STORE) as store:\n",
        "        store.put('fred/assets', combined_df)\n",
        "        print(f\"\\nData stored in {DATA_STORE} under 'fred/assets'\")\n",
        "\n",
        "    return combined_df\n",
        "\n",
        "# Fetch bond price indexes\n",
        "bond_price_indexes = fetch_bond_price_indexes()\n",
        "\n",
        "if bond_price_indexes is not None:\n",
        "    print(\"\\nBond price indexes fetched and stored successfully.\")\n",
        "else:\n",
        "    print(\"\\nFailed to fetch bond price indexes. Please check your internet connection and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KF-mh2nR1CP",
        "outputId": "907976ac-f1af-4f72-c930-788d97cdaf46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching bond price indexes from 2000-01-01 to 2024-07-25...\n",
            "Successfully fetched data for US Corp Master TRI\n",
            "Successfully fetched data for US High Yield TRI\n",
            "Successfully fetched data for Emerging Markets Corporate Plus TRI\n",
            "Successfully fetched data for 10-Year Treasury CMR\n",
            "\n",
            "Bond Price Indexes Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "DatetimeIndex: 6407 entries, 2000-01-03 to 2024-07-23\n",
            "Freq: B\n",
            "Data columns (total 4 columns):\n",
            " #   Column                               Non-Null Count  Dtype  \n",
            "---  ------                               --------------  -----  \n",
            " 0   US Corp Master TRI                   6329 non-null   float64\n",
            " 1   US High Yield TRI                    6329 non-null   float64\n",
            " 2   Emerging Markets Corporate Plus TRI  6329 non-null   float64\n",
            " 3   10-Year Treasury CMR                 6143 non-null   float64\n",
            "dtypes: float64(4)\n",
            "memory usage: 250.3 KB\n",
            "None\n",
            "\n",
            "Sample data:\n",
            "            US Corp Master TRI  US High Yield TRI  Emerging Markets Corporate Plus TRI  10-Year Treasury CMR\n",
            "DATE                                                                                                        \n",
            "2000-01-03              991.58             363.62                               114.64                  6.58\n",
            "2000-01-04              995.74             363.04                               114.57                  6.49\n",
            "2000-01-05              990.59             362.31                               114.49                  6.62\n",
            "2000-01-06              993.67             362.42                               114.55                  6.57\n",
            "2000-01-07              995.12             362.46                               114.68                  6.52\n",
            "\n",
            "Data stored in assets.h5 under 'fred/assets'\n",
            "\n",
            "Bond price indexes fetched and stored successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_path = 'assets.h5'\n",
        "destination_folder = '/content/drive/MyDrive/ML4T'\n",
        "destination_path = os.path.join(destination_folder, 'assets.h5')\n",
        "\n",
        "# Create the ML4T folder if it doesn't exist\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# Copy the file\n",
        "try:\n",
        "    shutil.copy(source_path, destination_path)\n",
        "    print(f\"Successfully copied {source_path} to {destination_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The source file {source_path} was not found.\")\n",
        "except PermissionError:\n",
        "    print(f\"Error: Permission denied. Unable to copy to {destination_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "# Verify the file was copied\n",
        "if os.path.exists(destination_path):\n",
        "    print(f\"Verified: {destination_path} exists in Google Drive\")\n",
        "else:\n",
        "    print(f\"Warning: {destination_path} was not found in Google Drive after copying\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdvaet0rSaL3",
        "outputId": "1b6f7be9-2a00-4680-e7f7-8c3d7116bff6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Successfully copied assets.h5 to /content/drive/MyDrive/ML4T/assets.h5\n",
            "Verified: /content/drive/MyDrive/ML4T/assets.h5 exists in Google Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnfzU8LJTqI_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}